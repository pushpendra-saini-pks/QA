Here is the updated `README.md` file with the Docker setup and the addition of the LLM model "llama3-8b-8192" to the tech stack:

---

# Q&A Application using RAG

## Overview üéôÔ∏è
This project provides a solution for processing PDF documents and allows for question-answering using embeddings stored in Pinecone. Users can upload a PDF file, and the system retrieves content based on similarity search while also generating answers through an LLM. The application uses the open-source model "llama3-8b-8192" for the generative AI component.

## Key Features ‚ú®
- **Document Chunking**: PDF files are chunked for better handling and processing.
- **Q&A System**: Users can ask questions based on the document, with a response generated by an LLM, along with relevant content retrieved by a similarity search.
- **Embeddings with Pinecone**: Document chunks are embedded into vector format and stored in Pinecone for fast retrieval.
- **Interactive UI**: The system provides an interactive interface where users can upload a PDF and interact with the generated content and the retrieved content side by side.

## Project Workflow üîÑ
1. **Upload PDF**: The user uploads a PDF file.
2. **Chunking**: The PDF is split into smaller text chunks.
3. **Embeddings Generation**: Text chunks are converted into vector embeddings.
4. **Storing Embeddings**: Embeddings are stored in Pinecone.
5. **Question Answering**: Users can ask questions, and the system retrieves relevant chunks and generates responses using the LLM.

## Project Architecture üèóÔ∏è
1. **PDF ‚Üí Text Chunks**: The uploaded PDF is chunked into smaller pieces of text.
2. **Embeddings Generation**: Text chunks are converted into vector embeddings.
3. **Embeddings Storage (Pinecone)**: Vector embeddings are stored in Pinecone.
4. **Interactive Q&A**: Users ask questions, and answers are generated by the LLM, combined with retrieved content based on similarity search.

## Requirements üìã
- Python 3.12 or higher
- Virtual environment management tool (venv, conda, etc.)
- All dependencies listed in `requirements.txt`

## Installation and Setup üõ†Ô∏è

### Local Setup
1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd <repository-folder>
   ```

2. **Create and activate virtual environment**:
   Using venv (Python's built-in virtual environment):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

   Using conda:
   ```bash
   conda create -n venv python=3.12
   conda activate venv
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Streamlit app**:
   ```bash
   streamlit run app.py
   ```

### Docker Setup üêã
1. **Build Docker Image**:
   ```bash
   docker build -t qna-rag-app .
   ```

2. **Run Docker Container**:
   ```bash
   docker run -p 8501:8501 --env GOOGLE_API_KEY=your-google-api-key --env PINECONE_API_KEY=your-pinecone-api-key --env GROQ_API_KEY=your-groq-api-key qna-rag-app
   ```

3. **Access the app**:
   Navigate to `http://localhost:8501` in your browser.

## Dockerfile üê≥
```Dockerfile
# app/Dockerfile

FROM python:3.12

COPY . /app
WORKDIR /app

RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    software-properties-common \
    git \
    && rm -rf /var/lib/apt/lists/*

RUN pip install -r requirements.txt

# Set environment variables
ENV GOOGLE_API_KEY=your-google-api-key
ENV PINECONE_API_KEY=your-pinecone-api-key
ENV GROQ_API_KEY=your-groq-api-key

EXPOSE 8501

HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

ENTRYPOINT ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

## Tech Stack üõ†Ô∏è
- **Groq**: Optimizing the Q&A process.
- **Langchain**: Modular and scalable components for the application.
- **Pinecone**: Vector database for embeddings storage and retrieval.
- **Streamlit**: Web application framework for the interactive user interface.
- **Llama3-8b-8192**: Open-source LLM for generating content based on user queries.
- **Tiktoken**: Used for tokenizing text for embeddings.
- **LangChain**: For creating embeddings and handling document processing.

## Troubleshooting üîç
If you encounter any issues:
1. Make sure your virtual environment is activated.
2. Verify Python version compatibility with `python --version`.
3. Ensure all API keys (Google, Pinecone, Groq) are set properly.

## License üìÑ
The source code for the project is licensed under the MIT license, which you can find in the LICENSE.md file.

## Contributing ü§ù
1. Fork the project.
2. Create your feature branch (`git checkout -b feature/AmazingFeature`).
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`).
4. Push to the branch (`git push origin feature/AmazingFeature`).
5. Open a Pull Request.
